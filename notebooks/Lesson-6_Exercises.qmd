---
title: "Tema 6: PEC"
format:
  html:
    code-copy:       true
    code-tools:      true
    df-print:        paged
    embed-resources: true
    theme:           ../www/extra-styles.scss
    toc:             true
    toc-location:    left
bibliography:        ../www/abd.bib
csl:                 ../www/apa-old-doi-prefix.csl
callout-appearance: minimal
---

# Introducción

En este tema hemos estudiado el concepto de **distribución predictiva** y cómo se puede estimar de manera sencilla mediante el método de Monte Carlo.

También hemos visto:

-   Cómo realizar comprobaciones predictivas con la **distribución predictiva posterior** (lo que llamamos **comprobaciones predictivas posteriores**, posterior predictive checks, o PPCs).

-   Cómo calcular **valores-p predictivos posteriores** para hacer inferencias y evaluar la discrepancia entre los datos observados y la distribución predictiva.

-   Cómo usar la **distribución predictiva previa** para evaluar la adecuación de la distribución previa a los datos observados.

En estos ejercicios, vamos a poner en práctica estos conceptos con algunos modelos ya conocidos y estudiados.
En este caso, vamos a utilizar los modelos beta-binomial y gamma-Poisson ya vistos en los temas anteriores.

Fíjate que @ross2022 asume distribuciones discreta (y no siempre uniformes) para el parámetro de probabilidad **en los ejemplos 7.1 a 7.4**.
Es decir, aunque la distribución de la variable observada sea binomial, **no se trata de modelos beta-binomiales**.

```{r setup}
#| message: false

# Paquetes:
library(tidyverse)
library(RColorBrewer)
library(scales)


# Configuración de la salida gráfica:

PALETA <- brewer.pal(8, "Set2") # Colores por defecto
color_defecto <- PALETA[1]      # Color por defecto
options(ggplot2.discrete.colour = PALETA)

theme_set(theme_bw()) # Tema "neutro" para la representación gráfica

# Redondea los números reales "inline":
options(digits = 3L)                
options(knitr.digits.signif = FALSE)

# Inicializa la semilla aleatoria:
set.seed(20250327)
```

Inicializamos el entorno como es habitual.
Dado que, además, vamos a utilizar el método de Monte Carlo, **hemos inicializado la semilla aleatoria**, para asegurar la **reproducibilidad de los resultados**.

# Ejercicio 1: Modelo beta-binomial de la "tasa de aceptación"

## Distribución predictiva previa

Vamos a empezar utilizando el ejemplo ya familiar que introdujimos en el Tema 3.

Recuerda que se trata de un modelo beta-binomial en el que el parámetro $\theta$ representa la "tasa de aceptación" de los/as usuari/as que han probado un app, a los que les pregunta si la descargarían en su móvil.

Los datos que se han obtenido en las dos muestras de la investigación son:

```{r beta-binomial-muestra}
aceptacion_muestra_1 <- tibble(
  id_participante   = 1:22,
  resp_descarga_app = c(
    "Si", "Si", "No", "No", "Si", "Si", "Si", "Si", "No", "Si", "Si",
    "Si", "Si", "Si", "Si", "Si", "No", "Si", "No", "Si", "Si", "Si"
  )
)

aceptacion_muestra_2 <- tibble(
  id_participante   = 1:113,
  resp_descarga_app = c(
    "Si", "Si", "No", "No", "Si", "Si", "Si", "Si", "No", "Si", "Si",
    "Si", "Si", "Si", "Si", "Si", "No", "Si", "No", "Si", "Si", "Si", 
    "No", "Si", "Si", "Si", "Si", "No", "No", "Si", "No", "Si", "Si", 
    "Si", "Si", "Si", "No", "Si", "No", "No", "Si", "No", "Si", "Si", 
    "No", "No", "No", "Si", "No", "No", "Si", "Si", "No", "No", "Si", 
    "No", "Si", "No", "No", "No", "Si", "Si", "No", "Si", "Si", "No", 
    "Si", "Si", "No", "Si", "Si", "No", "Si", "No", "Si", "No", "Si", 
    "No", "No", "No", "Si", "Si", "No", "No", "Si", "Si", "No", "No", 
    "No", "Si", "Si", "No", "Si", "Si", "No", "Si", "Si", "Si", "Si", 
    "No", "Si", "No", "No", "No", "No", "No", "Si", "No", "No", "Si", 
    "Si", "Si", "Si"
  )
)
```

Como en temas anteriores, vamos a utilizar una distribución no informativa para representar nuestra creencia a priori sobre la tasa de aceptación.

### Pregunta 1

-   Aproxima la distribución previa de $\theta$ por el método de Monte Carlo de manera que el valor esperado tenga una precisión de 0.01 con el 99% de probabilidad. Comprueba que la media y varianza se aproximan a los valores teóricos y representa la distribución resultante.

::: {#respuesta-1 .callout-note}
1) **Distribución utilizada:**
- Utilizaremos $Beta(1,1)$ como beta-binomial no informativa.
2) **Cálculo del número de muestras necesarias para prec. 0.01:**
- El error estándar de la media muestral es:

$$
\text{Error Estándar} = \sqrt{\frac{\sigma^2}{n}}
$$

- Donde la varianza de $Beta(\alpha, \beta)$ es:

$$
\sigma^2 = \frac{\alpha \cdot \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
$$

- Para un IC99%:

$$
n_{\text{sim}} = \left\lceil \frac{z_{0.99}^2 \cdot \sigma^2}{(\text{Error deseado})^2} \right\rceil
$$

- Donde:
  - $z_{0.99}$ es el valor crítico para el nivel de confianza del 99%
  - $\sigma^2$ es la varianza de la distribución Beta
  - $\text{Error deseado}$ es la precisión requerida en la media
  - $\lceil \cdot \rceil$ denota el techo (redondeo hacia arriba)

3) **Monte-Carlo para aproximar la distribución:**
- Simularemos muchos valores de $\theta$ a partir de la distribución $Beta(1,1)$.

4) **Calcula la media y varianza empíricas y compara con los valores teóricos:**
- El promedio empírico debería aproximar al valor esperado teórico de $Beta(1,1)$, que es 0.5.
- Lo mismo para la varianza.

```{r aproximación-distribución-previa-prec0.01}
# Parámetros de la Beta previa no informativa
alpha <- 1
beta <- 1

# Cálculo del número de simulaciones necesarias
# MCSE
var_teorica <- (alpha * beta) / (((alpha + beta)^2) * (alpha + beta + 1))
z_99 <- qnorm(0.995)  # Valor crítico para 99% bilateral
error_deseado <- 0.01
n_sim <- ceiling((z_99^2 * var_teorica) / (error_deseado^2))

# Simulación por Monte Carlo
theta_samples <- rbeta(n_sim, alpha, beta)

# Media y varianza empíricas
media_empirica <- mean(theta_samples)
var_empirica <- var(theta_samples)

# Valores teóricos
media_teorica <- alpha / (alpha + beta)
var_teorica

# Resultados
cat("Número de muestras:", n_sim, "\n")
cat("Media empírica:", media_empirica, "\n")
cat("Media teórica:", media_teorica, "\n")
cat("Varianza empírica:", var_empirica, "\n")
cat("Varianza teórica:", var_teorica, "\n")

# Estimación kernel de la densidad de las muestras Beta
densidad_beta <- density(theta_samples)

# Convertimos a tibble para tidyverse
densidad_beta_tbl <- tibble(
  x = densidad_beta$x,
  densidad = densidad_beta$y
)

# Añadimos la densidad teórica Beta(1,1)
densidad_beta_tbl |>
  mutate(dens_teorica = dbeta(x, alpha, beta)) |>
  ggplot(aes(x, densidad)) +
  geom_line(color = color_defecto) +
  geom_line(aes(y = dens_teorica), color = PALETA[6]) +
  labs(
    title = expression(paste("Densidad de Distribución Beta(1,1) Monte Carlo vs. Teórica")),
    x = expression(theta),
    y = "Densidad"
  )
```
:::

### Pregunta 2

-   A partir de la distribución previa simulada de $\theta$, simula los resultados de pruebas binomiales para la primera muestra del estudio. (Ten en cuenta que debe tener el tamaño muestral correspondiente). Representa la distribución predictiva previa resultante e interprétala.

::: {#respuesta-2 .callout-note}
- Partimos de la distribución previa simulada de valores de θ usando Monte Carlo *theta_samples*.

1) **Simulamos experimentos binomiales usando *theta_samples*:** Para cada valor de θ simulado, simulamos el número de *Si* en una muestra del mismo tamaño que la primera muestra (22 participantes)

2) **Obtenemos la distribución predictiva previa:** resultados simulados sobre el número de aceptaciones bajo la incertidumbre de θ.

3) **Representación mediante histograma para interpretar resultados simulados.**

```{r simula-pruebas-muestra}
n_muestra_1 <- length(aceptacion_muestra_1$id_participante)  # Tamaño de la primera muestra

# Simula la distribución predictiva previa
resultados_binomiales <- rbinom(length(theta_samples), size = n_muestra_1, prob = theta_samples)

# Visualización de la distribución predictiva previa
tabla_pred <- table(resultados_binomiales)
barplot(tabla_pred / sum(tabla_pred),
        main = "Distribución predictiva previa (n = 22)",
        xlab = "Número de aceptaciones simuladas",
        ylab = "Probabilidad",
        col = "skyblue",
        xlim = c(0, n_muestra_1 + 3))  # Añado 3 al limite superior porque el gráfico salía incompleto...
```

- Se observa una distribución casi plana en el gráfico, sin un valor claramente más probable, compatible con la distribución previa no informativa Beta(1,1).
:::

### Pregunta 3

-   Utilizando la distribución predictiva previa de la pregunta anterior, calcula en qué centil se encuentra la primera muestra empírica del estudio de aceptación. ¿Cuál es la probabilidad de obtener un valor igual o mayor que este? ¿Y un valor igual o menor?

::: {#respuesta-3 .callout-note}
1) **Contar *Si* la de muestra real.**

2) **Utilizar *resultados_binomiales***

3) **Calcular el centil empírico**

4) **Calcular probabilidad de obtener un valor igual o menor:**
- Proporción de simulaciones en las que el resultado es igual o mayor que el observado
- Proporción de simulaciones en las que el resultado es igual o menor que el observado

```{r calculo-centil+probabilidades}
# Cuenta de aceptaciones en la muestra real
n_aceptaciones_obs <- sum(aceptacion_muestra_1$resp_descarga_app == "Si")

# Centil empírico
centil <- floor(100 * mean(resultados_binomiales < # usa resultados_binomiales de simulación previa
                 n_aceptaciones_obs))

# Probabilidad de obtener igual o mayor
p_mayor_igual <- mean(resultados_binomiales >= n_aceptaciones_obs)

# Probabilidad de obtener igual o menor
p_menor_igual <- mean(resultados_binomiales <= n_aceptaciones_obs)

# Muestra los resultados
cat("Centil empírico:", centil, "\n")
cat("Probabilidad de obtener un valor igual o mayor:", p_mayor_igual, "\n")
cat("Probabilidad de obtener un valor igual o menor:", p_menor_igual, "\n")
```
A) **Centil empírico: `{r} centil`**
- El 74% de las simulaciones de la distribución predictiva previa dieron un número de aceptaciones menor que el observado en la muestra real.
- El valor observado está por encima de la mayoría de los resultados esperados bajo la previa no informativa.

B) **Probabilidad de obtener un valor igual o mayor: `{r} p_mayor_igual`**
- Indica que el resultado real está en el cuartil superior de la distribución predictiva previa, pero no es un valor extremadamente raro.

C) **Probabilidad de obtener un valor igual o menor: `{r} p_menor_igual`**
- Cerca del centil: la mayoría de los resultados simulados son iguales o menores que el observado.

Se concluye que:
- El resultado observado es totalmente compatible con la incertidumbre previa, aunque está por encima de la media previa (si el valor observado hubiera estado en los extremos podríamos sospechar que la previa no era adecuada o que el resultado es inusual).
- Bajo la previa no informativa, era más probable observar un número de aceptaciones menor que el que realmente se observó, pero el valor observado sigue siendo razonablemente probable.
:::

## Distribución predictiva posterior

### Pregunta 4

-   Utiliza el mismo nº de muestras de Monte Carlo de la distribución previa para aproximar la distribución posterior de $\theta$. (Utiliza la propiedad ya conocida de la conjugación para muestrear de la distribución posterior). Representa la distribución posterior obtenida.

::: {#respuesta-4 .callout-note}
1) **Calcula el nº de éxitos (*Si*) en la muestra real.**

2) **Actualiza los parámetros de $Beta(α, β)$ a Beta(α + k, β + n - k)$.**

3) **Simula en mismo número de muestras de la Beta posterior.**

4) **Representa la distribución resultante.**

```{r calculo-beta-posterior}
# Número de simulaciones usado previamente
n_sim <- length(theta_samples)  # theta_samples es el vector de la previa

# Datos observados ya calculados anteriormente
# n_aceptaciones_obs = k, éxitos
# n_muestra_1 = n, tamaño muestra

# Parámetros de la posterior
alpha_post <- alpha + n_aceptaciones_obs  # alpha ya fue definido como 1
beta_post <- beta + n_muestra_1 - n_aceptaciones_obs  # beta ya fue definido como 1

# Simulación Monte Carlo de la posterior
theta_post_samples <- rbeta(n_sim, alpha_post, beta_post)

# Estimación de densidades:
# Densidad previa ya estimada en densidad_beta
# Estimación de densidad para posterior
dens_post <- density(theta_post_samples)

# Tibble:
# Tibble para previa ya creado en densidad_beta_tbl. Añadimos grupo
densidad_beta_tbl <- densidad_beta_tbl |> mutate(grupo = "Previa")
# Convertimos a tibble y unimos para tidyverse
densidades <- bind_rows(
  densidad_beta_tbl,
  tibble(x = dens_post$x, densidad = dens_post$y, grupo = "Posterior")
)

# Gráfico comparativo
ggplot(densidades, aes(x = x, y = densidad, color = grupo, fill = grupo)) +
  geom_line(linewidth = 1) +
  geom_area(alpha = 0.2, position = "identity") +
  labs(
    title = expression(paste("Densidad de la distribución previa y posterior de ", theta, " (n = 22)")),
    x = expression(theta),
    y = "Densidad"
  ) +
  scale_fill_brewer(palette = "Set2")
```

- Se reduce la incertidumbre ya que la distribución posterior (línea azul) se concentra mucho más, mostrando una idea más detallada de cual es el valor probable de $\theta$.
- Posterior claramente desplazada hacia valores altos de $\theta$, aproximadamente entre 0.6 y 0.9, indicando una probabilidad de aceptación estimada alta.
:::

### Pregunta 5

-   A partir de la distribución posterior simulada de $\theta$, simula los resultados de pruebas binomiales para la primera muestra del estudio y represéntala.

::: {#respuesta-5 .callout-note}
1) **Utiliza muestras simuladas de la posterior de $\theta$ *theta_post_samples*.**

2) **Simula un experimento binomial para cada muestra de $\theta$.** Mismo tamaño muestral (n=22)

3) **Guarda los resultados de cada simulación.**

4) **Representa la distribución.**

```{r simulacion-resultados-binomiales}
# Simula la distribución predictiva posterior
resultados_binomiales_post <- rbinom(length(theta_post_samples), size = n_muestra_1,  # Ya tenemos theta_post_samples y n_muestra_1 definidos previamente
              prob = theta_post_samples)

# Visualización
tabla_post <- table(resultados_binomiales_post)
barplot(tabla_post / sum(tabla_post),
        main = "Distribución predictiva posterior (Muestra 1, n=22)",
        xlab = "Número de aceptaciones simuladas",
        ylab = "Probabilidad",
        col = "skyblue",
        xlim = c(0, n_muestra_1))  # Fijamos límite inferior en 0 ya que es un valor significativo en este contexto
```

- La distribución es mucho más concentrada que la predictiva previa. Ahora la mayor parte de la probabilidad está entre 13 y 20 aceptaciones.
- El valor más probable está cerca de 17-18 aceptaciones.
- Se reduce la incertidumbre. Valores extremos y valores menores a la mitad se vuelven muy poco probables, que antes eran igualmente plausibles bajo la previa no informativa.
:::

Lo que acabas de representar es la **distribución predictiva posterior** del modelo ajustado con la muestra 1 del estudio.

### Pregunta 6

-   Obten las distribuciones posterior y predictiva posterior con la muestra 2, **asumiendo desconocimiento total sobre la tasa de aceptación** (i.e., distribución no informativa).

::: {#respuesta-6 .callout-note}
**Repetimos el proceso para *aceptacion_muestra_2*.**

```{r distribucion-posterior+predictiva-muestra2}
# Datos observados en la muestra 2
n_aceptaciones_obs2 <- sum(aceptacion_muestra_2$resp_descarga_app == "Si")  # éxitos
n_muestra_2 <- nrow(aceptacion_muestra_2)  # tamaño muestra

# Simulación previa (opcional, si quieres comparar)
theta_prev_samples2 <- rbeta(n_sim, alpha, beta)  # parámetros ya definidos previamente

# Parámetros de la posterior
alpha_post2 <- alpha + n_aceptaciones_obs2
beta_post2 <- beta + n_muestra_2 - n_aceptaciones_obs2

# Simulación Monte Carlo de la posterior
theta_post_samples2 <- rbeta(n_sim, alpha_post2, beta_post2)

# Estimación de densidades previa y posterior
dens_prev_2 <- density(theta_prev_samples2)
dens_post_2 <- density(theta_post_samples2)

# Convertimos a tibble y unimos para tidyverse
densidades2 <- bind_rows(
  tibble(x = dens_post$x, densidad = dens_prev_2$y, grupo = "Previa"),
  tibble(x = dens_post$x, densidad = dens_post_2$y, grupo = "Posterior")
)

# Gráfico comparativo
ggplot(densidades2, aes(x = x, y = densidad, color = grupo, fill = grupo)) +
  geom_line(linewidth = 1) +
  geom_area(alpha = 0.2, position = "identity") +
  labs(
    title = expression(paste("Densidad de la distribución previa y posterior de ", theta, " (n = 113)")),
    x = expression(theta),
    y = "Densidad"
  )

# Simulación predictiva posterior
resultados_binomiales_post2 <- rbinom(n_sim, size = n_muestra_2, prob = theta_post_samples2)

# Visualización predictiva posterior
tabla_post2 <- table(resultados_binomiales_post2)
barplot(tabla_post2 / sum(tabla_post2),
        main = "Distribución predictiva posterior (Muestra 2, n=22)",
        xlab = "Número de aceptaciones simuladas",
        ylab = "Probabilidad",
        col = "orange",
        xlim = c(0, n_muestra_2))  # Fijamos límite inferior en 0 ya que es un valor significativo en este contexto
```

A) **Distribución posterior de θ de la muestra 2:**
- Muy estrecha y centrada aproximadamente en 0.6. Esto indica que tras observar los datos se tiene una esetimación muy precisa de la tasa de aceptación, con poca incertidumbre (valor de θ situado con gran probabilidad entre 0.5 y 0.7).

B) **Distribución predictiva posterior de la muestra 2**
- Simétrica y centrada cerca de 68-70 aceptaciones.
- Dispersión mucho menor que en la muestra 1 (a más datos, predicciones más precisas)
- Prácticamente no hay probabilidad de observar valores extremos (muy pocos o muchos *si*), mostrando gran confianza en la estimación.

En conclusión:
- Se observa mayor estrechez en la posterior y mayor concentración en la predictiva a comparación con la muestra 1.
- Esto se debe a la influencia del tamaño muestral (mayor cantidad de datos en la muestra 2), que hace que los datos dominen la inferencia previa no informativa y se pueda ajustar mejor la probabilidad de $\theta$ y hacer predicciones e inferencias.
:::

## Comprobaciones predictivas posteriores

### Pregunta 7

-   Dada la distribución posterior tras el ajuste del modelo con la muestra 2, aproxima la distribución predictiva posterior para un tamaño muestral de `{r} n_muestra_1`. Represéntala junto con la distribución predictiva posterior resultante de ajustar el modelo con la muestra 1, y representa mediante una línea vertical el valor obtenido de la muestra empírica 1.

::: {#respuesta-7 .callout-note}
1) **Se necesitan las distribuciones predictivas de las dos muestras para tamaño $n=22$:**
- Usa las muestras de la posterior de θ obtenidas con la muestra 2 en *theta_post_samples* y para cada θ simulado, genera un número de éxitos en una binomial de tamaño $n=22$.
- Para la muestra 1, recupera *resultados_binomiales_post*, que corresponde a la posterior de θ con la muestra 1 y tamaño 22.

2) **Calcula la frecuencia relativa de cada distribución predictiva:**
- Probabilidad empírica de cada posible número de *Si* simuladas en las dos distribuciones predictivas posteriores.
- Para comparar qué probabilidad asigna cada modelo a cada resultado posible (p.e. 10, 11, 12).

3) **Prepara los datos para la representación:**
- El valor total de *Si* obtenido en la muestra 1 ya está almacenado en *n_aceptaciones_obs*
- Define un rango común de valores para comparar creando un vector con todos los posibles valores de aceptaciones simuladas.
- Inicializa dos vectores de la misma longitud que el rango de valores para asegurar que ambas distribuciones sean representadas sobre el mismo eje aunque alguna de ellas no tenga simulaciones para todos los valores posibles (aparecería con frecuencia 0).
- Rellena los vectores de frecuencias con las probabilidades empíricas calculadas para cada valor, listos para comparar.

4) **Representa ambas distribuciones predictivas posteriores en el mismo gráfico.**

```{r binomial-posterior-n22+representacion-m1+m2}
# Simular la predictiva posterior para n = 22 usando la posterior de muestra 2
resultados_binomiales_post2_n22 <- rbinom(length(theta_post_samples2), size = n_muestra_1, prob = theta_post_samples2) # n_muestra_1 ya calculado

# Predictiva posterior muestra 1 = resultados_binomiales_post

# Preparar las frecuencias relativas para ambas distribuciones
tabla_post1 <- table(resultados_binomiales_post) / length(resultados_binomiales_post)
tabla_post2_n22 <- table(resultados_binomiales_post2_n22) / length(resultados_binomiales_post2_n22)

# Define el rango común de valores
valores <- min(as.numeric(names(tabla_post1)), as.numeric(names(tabla_post2_n22))):
           max(as.numeric(names(tabla_post1)), as.numeric(names(tabla_post2_n22)))
freq_post1 <- rep(0, length(valores))
freq_post2 <- rep(0, length(valores))
names(freq_post1) <- names(freq_post2) <- as.character(valores)
freq_post1[names(tabla_post1)] <- tabla_post1
freq_post2[names(tabla_post2_n22)] <- tabla_post2_n22

# Representación conjunta
plot(valores, freq_post1, type = "h", lwd = 12, lend = 1, col = "skyblue",
     xlab = "Número de aceptaciones simuladas",
     ylab = "Probabilidad",
     main = "Comparación de distribuciones predictivas posteriores (n = 22)",
     ylim = c(0, max(freq_post1, freq_post2)))
lines(valores, freq_post2, type = "h", lwd = 6, lend = 1, col = "orange")
abline(v = n_aceptaciones_obs, col = "red", lwd = 3)  # Para añadir la línea vertical indicando el valor observado en la muestra 1, ya guardado en n_aceptaciones_obs
legend("topleft",
       legend = c("Ajuste muestra 1", "Ajuste muestra 2", "Valor empírico muestra 1"),
       col = c("skyblue", "orange", "red"), lwd = c(6, 3, 2))
```

A) **Distribuciones predictivas posteriores:**
- Ambas distribuciones tienen una dispersión similar ya que estamos simulando el mismo tamaño muestral (n=22) en ambos casos.
- Diferencias en la ubicación: mientras que el ajuste con muestra 1 (azul) es más desplazado hacia valores altos de aceptaciones (en torno a 17), la distribución ajustada con la muestra 2 (naranja) está centrada en valores más bajos (aprox. 13).

B) **Valor empírico de la muestra 1:**
- Mejor alineado con distribución predictiva ajustada a la propia muestra 1. Algo completamente esperable ya que este modelo se ajustó específicamente a esos datos.

Conclusiones:
- Es natural que un modelo prediga mejor los datos con los que fue ajustado.
- Diferentes conjuntos de datos pueden llevar a diferentes inferencias sobre el mismo parámetro.
- La diferencia en las tasas estimadas se debe al origen de los datos. En principio, trabajar con la muestra más grande (muestra 2) debería proporcionar una estimación más precisa del parámetro poblacional real aún cuando ajustamos el modelo a una muestra menor (n = 22).
:::

### Pregunta 8

-   Calcula, en el modelo ajustado con la muestra 2, la probabilidad de obtener un valor mayor o igual / menor o igual que la primera muestra empírica. ¿Cómo se representan estas probabilidades en el gráfico anterior?

::: {#respuesta-8 .callout-note}
```{r probabilidad-valor-muestra2-vs-muestra1}
# Calcular probabilidades:
p_mayor <- mean(resultados_binomiales_post2_n22 > n_aceptaciones_obs)  # Estrictamente mayor
p_menor <- mean(resultados_binomiales_post2_n22 < n_aceptaciones_obs)  # Estrictamente menor
p_igual <- mean(resultados_binomiales_post2_n22 == n_aceptaciones_obs) # Exactamente igual

# Probabilidad de obtener un valor mayor o igual
prob_mayor_igual <- p_mayor + p_igual 
cat("Probabilidad de obtener un valor mayor o igual:", prob_mayor_igual, "\n")

# Probabilidad de obtener un valor menor o igual
prob_menor_igual <- p_menor + p_igual
cat("Probabilidad de obtener un valor menor o igual:", prob_menor_igual, "\n")

# Representación gráfica
plot(valores, freq_post2, type = "h", lwd = 8, lend = 1, col = "orange",
     xlab = "Número de aceptaciones simuladas",
     ylab = "Probabilidad",
     main = "Probabilidades bajo el modelo ajustado con muestra 2",
     ylim = c(0, max(freq_post2) * 1.2))
abline(v = n_aceptaciones_obs, col = "red", lwd = 4)  # Añadimos el valor empírico
# Sombreamos el área para valores mayores o iguales
for (i in which(as.numeric(names(freq_post2)) >= n_aceptaciones_obs)) {
  rect(as.numeric(names(freq_post2))[i] - 0.4, 0, 
       as.numeric(names(freq_post2))[i] + 0.4, freq_post2[i],
       col = rgb(1, 0, 0, 0.3), border = NA)
}
# Añadimos texto con las probabilidades
text(n_aceptaciones_obs + 3, max(freq_post2) * 0.9, 
     paste("P(X ≥", n_aceptaciones_obs, ") =", round(prob_mayor_igual, 3)),
     col = "darkred")
text(n_aceptaciones_obs - 3, max(freq_post2) * 0.8, 
     paste("P(X ≤", n_aceptaciones_obs, ") =", round(prob_menor_igual, 3)),
     col = "darkblue")
# Leyenda
legend("topleft", 
       legend = c("Ajuste muestra 2", "Valor empírico muestra 1", "P(X ≥ valor empírico)"),
       col = c("orange", "red", rgb(1, 0, 0, 0.3)), 
       lwd = c(8, 4, 12))
```

A) **Gráfico:**
- Igual que antes las barras naranjas representan la distribución predictiva posterior para un tamaño muestral $n=22$ utilizando la muestra 2. La distribución está centrada alrededor de 13 *Si*.
- El valor empírico de la muestra 1 está marcado por la línea roja (17 respuestas afirmativas).
- El área sombreada en rosa representa la probabilidad de obtener un valor mayor o igual al empírico.

B) **Valores numéricos:**
- P(X ≥ 17) = `{r} prob_mayor_igual`: Solo el `{r} round(100 * prob_mayor_igual, 1)`% de las simulaciones basadas en el modelo ajustado con la muestra 2 dieron 17 o más aceptaciones.
- P(X ≤ 17) = `{r} prob_menor_igual`: El `{r} round(100 * prob_menor_igual, 1)`% de las simulaciones dieron 17 o menos aceptaciones.

Interpretación:
- Existe una discrepancia entre lo que predice el modelo ajustado con la muestra 2 y lo observado en la muestra 1.
- La tasa de aceptación en la muestra 1 parece ser más alta que la estimada a partir de la muestra 2.
- El mayor tamaño muestral de la muestra 2 ($n = 113$) tiene importantes implicaciones:
1) **Mayor precisión en la estimación de θ:** Al tener más datos, la distribución posterior de θ basada en la muestra 2 es mucho más precisa y estrecha, lo que la hace más fiable.
2) **Efecto en la distribución predictiva:** Aunque simulamos un mismo tamaño muestral ($n = 22$), al partir de datos más precisos, la distribución predictiva basada en la muestra 2 es más fiable sobre dónde deberían caer los resultados reales de la población estudiada.
:::

### Pregunta 9

-   Si te preguntasen por el *valor-*$p$ *predictivo posterior* de la hipótesis que "la muestra 1 esté extraída de la misma población que la muestra 2", ¿qué valor reportarías y cómo lo interpretarías?

::: {#respuesta-9 .callout-note}
La elección del *valor-*$p$ *predictivo posterior* a usar (contraste unilateral en una determinada dirección, bilateral...) depende fundamentalmente del contexto de los datos y de la pregunta que nos hacemos.

1) **Detectamos la dirección de la anomalía:**
- Si se cumple *media_predictiva < n_aceptaciones_obs*: El modelo subestima el número de aceptaciones, por lo que nos interesa la probabilidad de obtener valores tan altos o más altos que el observado (P(X ≥ *n_aceptaciones_obs*)). 
- Si *media_predictiva > n_aceptaciones_obs*: El modelo sobreestima el número de aceptaciones, por lo que nos interesa la probabilidad de obtener valores tan bajos o más bajos que el observado (P(X ≤ *n_aceptaciones_obs*)).

- Al elegir la dirección del contraste basada en la discrepancia observada, aumentamos la sensibilidad para detectar este tipo específico de desviación

```{r valor-p-pred2-muestra1}
# Comparar las medias/datos
media_predictiva <- mean(resultados_binomiales_post2_n22)
# La comparación se hace con el nº de aceptaciones: n_aceptaciones_obs

# Elegir el contraste unilateral según la dirección de la discrepancia
if (media_predictiva < n_aceptaciones_obs) {
  # Si el modelo predice menos aceptaciones que las observadas
  valor_p_pred_2_muestra1 <- mean(resultados_binomiales_post2_n22 >= n_aceptaciones_obs)
  direccion <- "mayor"
} else {
  # Si el modelo predice más aceptaciones que las observadas
  valor_p_pred_2_muestra1 <- mean(resultados_binomiales_post <= n_aceptaciones_obs)
  direccion <- "menor"
}

# Mostrar resultados
cat("Media predictiva:", media_predictiva, "\n")
cat("Media empírica (observada):", n_aceptaciones_obs, "\n")
cat("El valor observado es", direccion, "que lo esperado bajo el modelo\n")
cat("Valor-p predictivo posterior:", valor_p_pred_2_muestra1, "\n")
```

Un valor-p predictivo posterior de `{r} valor_p_pred_2_muestra1`indica que, bajo el modelo ajustado con la muestra 2, hay un `{r} 100 * valor_p_pred_2_muestra1` de posibilidades de observar un número de aceptaciones igual o mayor al observado en la muestra 1, lo que sugiere que el valor observado en la muestra 1 es inusualmente alto comparado con lo que predice el modelo ajustado con la muestra 2.

En términos prácticos, esto sugiere que la muestra 1 muestra una tasa de aceptación mayor de lo que esperaríamos si proviniera de la misma población que la muestra 2. El resultado es relativamente inusual bajo la hipótesis de que ambas muestras provienen de la misma población, pero no es extremadamente raro ya que no es menor a 0.05. Podemos decir que hay cierta discrepancia pero no una incompatibilidad clara.
:::

### Pregunta 10

-   Prueba a hacerlo a la inversa; es decir, ajusta el modelo con la muestra 1, y después realiza la *comprobación predictiva posterior* de si la muestra 2 proviene de la misma población que la muestra 1. ¿Qué conclusión obtendrías?

::: {#respuesta-10 .callout-note}
```{r probabilidad-valor-muestra1-vs-muestra2}
# Ya tenemos theta_post_samples (posterior ajustada con muestra 1)
# También n_muestra_2 (número de observaciones de la muestra 2)

# Simular la predictiva posterior para n = 113
resultados_binomiales_post1_n113 <- rbinom(length(theta_post_samples), size = n_muestra_2, prob = theta_post_samples)

# Valor empírico de la muestra 2 = n_aceptaciones_obs2

# Comparar las medias
media_predictiva2 <- mean(resultados_binomiales_post1_n113)
# La comparación se hace con el nº de aceptaciones: n_aceptaciones_obs2

# Elegir el contraste unilateral según la dirección de la discrepancia
if (media_predictiva2 < n_aceptaciones_obs2) {
  # Si el modelo predice menos aceptaciones que las observadas
  valor_p_pred_1_muestra2 <- mean(resultados_binomiales_post1_n113 >= n_aceptaciones_obs2)
  direccion <- "mayor"
} else {
  # Si el modelo predice más aceptaciones que las observadas
  valor_p_pred_1_muestra2 <- mean(resultados_binomiales_post1_n113 <= n_aceptaciones_obs2)
  direccion <- "menor"
}

# Mostrar resultados
cat("Media predictiva:", media_predictiva2, "\n")
cat("Media empírica (observada):", n_aceptaciones_obs2, "\n")
cat("El valor observado es", direccion, "que lo esperado bajo el modelo\n")
cat("Valor-p predictivo posterior:", valor_p_pred_1_muestra2, "\n")
```

Un valor-p predictivo posterior de `{r} valor_p_pred_1_muestra2` indica que, bajo el modelo ajustado con la muestra 1, hay un `{r} 100 * valor_p_pred_1_muestra2` de posibilidades de observar un número de aceptaciones igual o menor al observado en la muestra 2, lo que sugiere que nuestro modelo estaría sobreestimando el número de *si* de la población basándonos en los resultados de la muestra 2.

El valor obtenido es *valor-*$p$ *< 0.05*, lo que llevaría a rechazar $h_0$, es decir, concluiríamos que la muestra 2 proviene de una población diferente a la muestra 1.
:::

# Ejercicio 2: Modelo gamma-Poisson de la "tasa de fertilidad"

El ejercicio anterior se basa en la distribución beta-binomial, que permite simplificar la distribución predictiva posterior al necesitar generar únicamente un valor observado (nº de usuarios que "aceptan" la aplicación) para cada muestra.
Sin embargo, es habitual encontrar distribuciones predictivas posteriores más complejas o derivadas, como hemos visto en la lectura.
En el siguiente ejemplo veremos cómo simular muestras de una distribución predictiva posterior utilizando el modelo "gamma-Poisson".

## Distribución predictiva posterior

En [la lectura del Tema 5](https://agora.uned.es/mod/resource/view.php?id=512338) (@hoff2009) y los ejercicios vimos el ejemplo de las tasas de fertilidad de mujeres de 40 años con y sin título universitario, con datos de la Encuesta Social General de los EEUU durante la década de los 1990 [los detalles están en @hoff2009, capítulo 3].

A continuación tienes los datos que aparecen en la lectura, los estadísticos resumen para cada grupo, y una representación gráfica:

```{r datos-fertilidad-gss-1990}
fertilidad_gss_1990 <- tibble(
  titulo_uni = c("sin" |> rep(7),                 "con" |> rep(5)),
  n_hijos    = c(0:6,                             0:4),
  frecuencia = c(20L, 19L, 38L, 20L, 10L, 2L, 2L, 11L, 11L, 13L, 7L, 2L)
) |>
  # Rellena los niveles para hacer ambas muestras más "comparables":
  complete(titulo_uni, n_hijos, fill = list(frecuencia = 0))

fert_estadisticos <- fertilidad_gss_1990 |>
  group_by(titulo_uni) |>
  summarize(y = sum(n_hijos * frecuencia), n = sum(frecuencia))

fert_estadisticos # y = nº hijos en cada grupo, n = nº mujeres en cada grupo

fertilidad_gss_1990 |>
  ggplot(aes(n_hijos, frecuencia, fill = titulo_uni)) +
  geom_col(position = "dodge") +
  labs(fill = "Título universitario", x  = "Nº hijos", y = "Frecuencia") +
  scale_fill_brewer(palette = "Set2")
```

La distribución posterior de la tasa de fertilidad $\lambda$ en el modelo gamma-Poisson puede obtenerse mediante conjugación de la distribución previa $\lambda \sim Gamma(a, b)$, y viene dada por $\lambda \sim Gamma(a + \sum y_i, b + n)$, siendo $\sum y_i$ el nº total de ocurrencias observadas en una muestra (en nuestro caso, nº total de hijos en la muestra / cada grupo) y $n$ el nº total de casos (nº de mujeres la muestra / en cada grupo).

Como vimos en los ejercicios del tema 5, las distribuciones posteriores para cada grupo, asumiendo una distribución previa $\lambda \sim Gamma(2, 1)$, vienen dadas por:

```{r fertilidad-ajuste}
A_PRE <- 2L
B_PRE <- 1L

params_fertilidad <- fert_estadisticos |> mutate(
  a_post = A_PRE + y,
  b_post = B_PRE + n
)

params_fertiliad_sin <- params_fertilidad |>
  filter(titulo_uni == "sin") 
a_post_sin <- params_fertiliad_sin |> pull(a_post)
b_post_sin <- params_fertiliad_sin |> pull(b_post)

params_fertiliad_con <- params_fertilidad |>
  filter(titulo_uni == "con") 
a_post_con <- params_fertiliad_con |> pull(a_post)
b_post_con <- params_fertiliad_con |> pull(b_post)
```

$$
  (\lambda | y_{sin}) \sim Gamma(`{r} a_post_sin`, `{r} b_post_sin`)
$$

$$
  (\lambda | y_{con}) \sim Gamma(`{r} a_post_con`, `{r} b_post_con`)
$$

### Pregunta 11

-   Utilizando 10^6^ muestras simuladas, aproxima las dos distribuciones posteriores y represéntalas.

*(Nota: Para representar una densidad directamente con `ggplot()` a partir de las muestras de simuladas, consulta la ayuda de `geom_density()`)*

::: {#respuesta-11 .callout-note}
1) **Identifica los parámetros de las distribuciones gamma posteriores para cada grupo:**
- Mujeres sin título: $\lambda_{sin} \sim \text{Gamma}(219, 112)$
- Mujeres con título: $\lambda_{con} \sim \text{Gamma}(68, 45)$

2) **Simula muestras de ambas distribuciones usando *rgamma()* y el tamaño de muestra pedido de $n=10^6$.**

3) **Representa gráficamente.**

```{r aproximacion-distribuciones-gamma}
# Simular 10^6 muestras de cada distribución posterior usando a_post_sin, b_post_sin, a_post_con, b_post_con
n_sim <- 1e6
lambda_sin <- rgamma(n_sim, shape = a_post_sin, rate = b_post_sin)
lambda_con <- rgamma(n_sim, shape = a_post_con, rate = b_post_con)

# Preparar los datos para ggplot
df_lambda <- bind_rows(
  tibble(lambda = lambda_sin, grupo = "Sin título universitario"),
  tibble(lambda = lambda_con, grupo = "Con título universitario")
)

# Representar las densidades
ggplot(df_lambda, aes(x = lambda, fill = grupo, color = grupo)) +
  geom_density(alpha = 0.4) +
  labs(
    title = "Distribuciones posteriores de la tasa de fertilidad (\u03BB)",
    x = "Tasa de fertilidad (\u03BB)",
    y = "Densidad",
    fill = "Grupo",
    color = "Grupo"
  ) +
  scale_fill_brewer(palette = "Set2")
```

A) **Diferencias en la posición de las distribuciones:**
- La distribución para mujeres SIN título universitario (azul) está claramente desplazada hacia la derecha, indicando una tasa de fertilidad posterior más alta respecto la tasa de mujeres CON título (naranja), desplazada a la izquierda.

B) **Solapamiento entre las distribuciones:**
- Aunque hay cierto solapamiento, la mayor parte de la densidad de cada grupo está bien separada.
- Sugiere, con gran probabilidad, que las tasas de fertilidad difieren entre grupos.

C) **Dispersión de las distribuciones:**
- Ambas distribuciones son relativamente estrechas, lo que indica que la incertidumbre sobre la tasa de fertilidad es baja, gracias al tamaño de muestra y la información que aporta.

:::

### Pregunta 12

-   A partir de las distribuciones posteriores de $\lambda$, aproxima las distribuciones predictivas posteriores simulando datos de la distribución de Poisson (consulta la ayuda de `rpois()` si lo necesitas). Representa las distribuciones predictivas posteriores de ambos grupos.

::: {#respuesta-12 .callout-note}
*(NOTA: Aclaraciones)*
- *lambda_sin* y *lambda_con* son muestras simuladas de la tasa de fertilidad ($\lambda$) para cada grupo usando *rgamma()*, pero es un parámetro, no un dato observable directamente.
- En este caso *rgamma()* genera un vector de *n_sim* valores aleatorios de la distribución gamma con los parámetros dados donde cada valor simulado es una posible tasa de fertilidad simulada de la distribución posterior, pero no nos dice cuántos hijos tendrá una mujer.
- Para representar teoricamente la distribución usamos *geom_density* que da una aproximación a la densidad de la simulación (pares *x=valor de lambda*, *y=densidad*).
- También sirve para calcular cualquier cantidad resumen (media, mediana, percentiles, etc) directamente de las simulaciones o usar los valores simulados para generar un dato nuevo.
- En este caso utilizamos *rpois()* para generar un posible número de hijos simulando una mujer nueva con cada tasa de fertilidad generada por los datos de *lambda_sin* y *lambda_con*.
- La Poisson es una distribución de probabilidad discreta que describe el número de veces que ocurre un evento en un intervalo fijo de tiempo o espacio, si estos eventos ocurren con una tasa promedio constante y de manera independiente. En nuestro caso el evento es "tener un hijo" y la tasa promedio es $\lambda$.
- Se elije gamma para la posterior porque es la distribución conjugada de la Poisson. Esto significa que, si partimos de una gamma para $\lambda$ y observamos datos Poisson, la distribución posterior de $\lambda$ también será una gamma.
*(fin de aclaraciones)*

```{r distribuciones-predictivas-posteriores-Poisson}
# Simular datos de la distribución de Poisson para cada muestra de λ con lambda_sin y lambda_con (muestras posteriores de λ para cada grupo)
# Esto genera una muestra de la distribución predictiva posterior para cada grupo
y_pred_sin <- rpois(length(lambda_sin), lambda_sin)
y_pred_con <- rpois(length(lambda_con), lambda_con)

# Preparar los datos para la grafica
df_pred <- bind_rows(
  tibble(n_hijos = y_pred_sin, grupo = "Sin título universitario"),
  tibble(n_hijos = y_pred_con, grupo = "Con título universitario")
)

# Representar las distribuciones predictivas posteriores
ggplot(df_pred, aes(x = n_hijos, fill = grupo, color = grupo)) +
  geom_bar(aes(y = after_stat(prop)), position = "dodge", stat = "count", alpha = 0.5) +
  labs(
    title = "Distribuciones predictivas posteriores del número de hijos",
    x = "Número de hijos",
    y = "Proporción",
    fill = "Grupo",
    color = "Grupo"
  ) +
  scale_fill_brewer(palette = "Set2")
```

:::

## Inferencia sobre la distribución predictiva posterior

En base a las distribuciones predictivas posteriores, obtén las respuetas a continuación.

### Pregunta 13

-   ¿Cuáles son las probabilidades de que una mujer (de 40 años en los 90 en USA) con 4 hijos o más sea o no titulada universitaria? ¿Cuál es la "odds" de que no sea titulada universitaria?

::: {#respuesta-13 .callout-note}
**1) Probabilidad de que una mujer con 4 hijos o más sea/no sea titulada universitaria:**
$$P(\text{titulada} \mid \text{nº hijos} \geq 4)$$
$$P(\text{no titulada} \mid \text{nº hijos} \geq 4)$$
Odds de que no sea titulada universitaria
$$\text{odds} = \frac{P(\text{no titulada} \mid \text{nº hijos} \geq 4)}{P(\text{titulada} \mid \text{nº hijos} \geq 4)}$$
```{r probabilidad+odds-mujer-4hijos}
# Contamos cuántas simulaciones tienen 4 o más hijos en cada grupo
n_4mas_con <- sum(y_pred_con >= 4)
n_4mas_sin <- sum(y_pred_sin >= 4)
n_4mas_total <- n_4mas_con + n_4mas_sin

# Calculamos las probabilidades condicionales
p_titulo_dado_4mas <- n_4mas_con / n_4mas_total
p_no_titulo_dado_4mas <- n_4mas_sin / n_4mas_total

# Calculamos las odds
odds_no_titulo <- p_no_titulo_dado_4mas / p_titulo_dado_4mas

# Mostramos resultados
cat("P(titulada | ≥4 hijos):", p_titulo_dado_4mas, "\n")
cat("P(no titulada | ≥4 hijos):", p_no_titulo_dado_4mas, "\n")
cat("Odds de no ser titulada universitaria:", odds_no_titulo, "\n")
```
:::

### Pregunta 14

-   Si tomamos dos mujeres al azar, una con y otra sin titulación universitaria, ¿cuál es la probabilidad de que la mujer con titulación universitaria tenga más hijos que la mujer sin titulación universitaria?

::: {#respuesta-14 .callout-note}
```{r probabilidad-titulo-más-hijos-sin}
# Probabilidad de obtener un valor mayor o igual
prob_con_mas_hijos <- mean(y_pred_con > y_pred_sin)

cat("Probabilidad de que la mujer con titulación universitaria tenga más hijos que la sin titulación:", prob_con_mas_hijos, "\n")
```
:::

### Pregunta 15

-   A partir de estas aproximaciones a las distribuciones predictivas posteriores, ¿podrías obtener la probabilidad conjunta de que una mujer no tenga ningún hijo y sea o no titulada universitaria? Justifica tu respuesta.

::: {#respuesta-15 .callout-note}
*(NOTA: Para calcular probabilidades conjuntas no hay que usar proporciones fijas de la muestra; hay que incorporar la incertidumbre en todos los parámetros: probabilidad de tener título universitario y tasa de fertilidad condicionada a tener título)*

1) **Modelar incertidumbre sobre la proporción de mujeres con título universitario**
- La proporción de mujeres con título universitario en la población es un parámetro desconocido con incertidumbre que se ha de simular.

```{r proporcion-mujeres-con-título}
# Extraemos estadísticos a utilizar ahora y más adelante
n_con <- fert_estadisticos |> filter(titulo_uni == "con") |> pull(n)  # mujeres con título universitario sin hijos
n_total <- sum(fert_estadisticos$n)  # mujeres sin hijos

# Usamos un modelo Beta-Binomial para esta proporción con previa no informativa Beta(1,1)
# Parámetros de la posterior para la proporción de mujeres con título
alpha_post_prop <- alpha + n_con
beta_post_prop <- beta + (n_total - n_con)

# Simulamos de la posterior de la proporción de mujeres con título
n_sim <- 1e6
prop_titulo_sim <- rbeta(n_sim, alpha_post_prop, beta_post_prop) # vector de probabilidades simuladas de tener un título
```

2) **Recordar las posteriores simuladas para las tasas de fertilidad en cada grupo:**
- Mujeres con título: Gamma(68, 45) recogido en *lambda_con*
- Mujeres sin título: Gamma(219, 112) recogido en *lambda_sin*

3) **Simulación conjunta para cada observación:**
- Primero se determinará aleatoriamente si tiene título.
- Luego se generará el número de hijos según la tasa de fertilidad correspondiente.

```{r simulacion-conjunta-observaciones}
# Para cada simulación, determinamos si tiene título y asignamos un número de hijos acorde a su grupo
resultados_sim <- tibble(
  tiene_titulo = rbinom(n_sim, 1, prop_titulo_sim),
  n_hijos = ifelse(tiene_titulo == 1, y_pred_con, y_pred_sin) # asigna valor y_pred_con[i] si tiene_titulo == 1 y y_pred_sin[i] si tiene_titulo == 0
)
```

4) **Cálculo de probabilidades conjuntas**

```{r probabilidades-conuntas-0hijos-estudios}
# Probabilidad conjunta: tener 0 hijos y título universitario
prob_n_con <- mean(resultados_sim$n_hijos == 0 & resultados_sim$tiene_titulo == 1)

# Probabilidad conjunta: tener 0 hijos y no tener título
prob_n_sin <- mean(resultados_sim$n_hijos == 0 & resultados_sim$tiene_titulo == 0)

# Presentamos resultados
tibble(
  grupo = c("Con título", "Sin título"),
  prob_0_hijos = c(prob_n_con, prob_n_sin)
)
```
:::

## Comprobaciones predictivas posteriores

### Pregunta 16

-   Representa la *proporción* de mujeres tituladas universitarias en función del número de hijos, junto con su distribución predictiva posterior.

::: {#respuesta-16 .callout-note}
```{r proporcion-tituladas}
# Datos empíricos: distribución del número de hijos en mujeres con título
datos_empiricos <- fertilidad_gss_1990 |>
  filter(titulo_uni == "con") |>
  mutate(proporcion = frecuencia / sum(frecuencia)) |>
  select(n_hijos, proporcion)

# Datos predictivos: distribución posterior del número de hijos en mujeres con título
datos_predictivos <- tibble(n_hijos = y_pred_con) |>
  count(n_hijos) |>
  mutate(proporcion = n / sum(n)) |>
  select(n_hijos, proporcion)

# Preparar datos para gráfico
datos_empiricos <- datos_empiricos |> mutate(tipo = "Empírica")
datos_predictivos <- datos_predictivos |> mutate(tipo = "Predictiva")

datos_plot <- bind_rows(
  datos_empiricos,
  datos_predictivos
)

# Representación gráfica
ggplot(datos_plot, aes(x = factor(n_hijos), y = proporcion, fill = tipo)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8) +
  labs(
    title = "Distribución del número de hijos en mujeres con título universitario",
    x = "Número de hijos",
    y = "Proporción",
    fill = "Distribución"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_fill_brewer(palette = "Set2")
```

Conclusiones:
- La proporción predictiva sobreestima la proporción de mujeres con 1 hijo en aproximadamente un 7,5% y subestima las mujeres con 2 hijos sobre un 5%, lo que hace que el modelo otorgue 1 hijo a la mayor la mayor proporción de mujeres mientras que los datos muestran que son las de 2 hijos. El resto de categorías hace una predicción bastante ajustada a la muestra.
- El modelo predictivo extiende la predicción hasta 7 hijos, más allá de lo observado en la muestra. No se si esto es bueno o malo... En caso de que en la población real sí hubiera casos de mujeres con 7 hijos que no han sido capturados en la muestra tendría sentido. Si no los hay, puede ser algo negativo.
- Se necesitaría un mayor tamaño muestral para poder llegar a una buena conclusión al respecto.
:::

## Comprobaciones predictivas posteriores sobre la muestra

```{r n-muestra-con}
# Se extrae aquí un valor para utilizar más adelante
n_con <- fert_estadisticos |> filter(titulo_uni == "con") |> pull(n)
```

Para hacer comprobaciones predictivas, no basta con aproximar una muestra predictiva posterior.
Como has podido ver en la lectura, necesitamos obtener estimadores de dicha distribución con los que poder comparar estadísticos de la distribución muestra.

Para ello, en lugar de aproximar la distribución predictiva posterior mediante muestras de Monte Carlo, lo que necesitamos es obtener la distribución predictiva posterior del estadístico de con el que queremos comparar la muestra empírica.
Es decir, necesitamos generar "muestras empíricas simuladas", calcular ese mismo estadístico, y compararlo con el estadístico de la muestra empírica.

A continuación vamos a hacer eso mismo con las distribuciones predictivas posteriores de los dos grupos de la población estudiada

### Pregunta 17

-   Observa el máximo número de hijos que se obtiene en la distribución empírica y en la distribución predictiva posterior en la pregunta 16. ¿Cuánto es en cada caso?

::: {#respuesta-17 .callout-note}
1) **Para la distribución empírica:** 
Revisar los datos originales de *fertilidad_gss_1990* para identificar el máximo número de hijos observado en mujeres con título universitario.

2) **Para la distribución predictiva posterior:** Examinar los valores máximos en nuestras simulaciones *y_pred_con* y *y_pred_sin*.

```{r maximo-numero-hijos}
# Máximo número de hijos en la distribución empírica
max_hijos_emp <- fertilidad_gss_1990 %>%
  filter(titulo_uni == "con", frecuencia > 0) %>%
  pull(n_hijos) %>%
  max()

max_hijos_emp_sin <- fertilidad_gss_1990 %>%
  filter(titulo_uni == "sin", frecuencia > 0) %>%
  pull(n_hijos) %>%
  max()

# Máximo número de hijos en la distribución predictiva posterior
max_pred_con <- max(y_pred_con)
max_pred_sin <- max(y_pred_sin)

# Mostrar resultados
cat("Distribución empírica:\n")
cat("- Máximo número de hijos (con título):", max_hijos_emp, "\n")
cat("- Máximo número de hijos (sin título):", max_hijos_emp_sin, "\n\n")

cat("Distribución predictiva posterior:\n")
cat("- Máximo número de hijos (con título):", max_pred_con, "\n")
cat("- Máximo número de hijos (sin título):", max_pred_sin, "\n")
```

- Hay tanta diferencia porque la predictiva posterior incorpora, además de la información observada, la incertidumbre sobre la tasa de fertilidad (λ) y la variabilidad natural de la Poisson. 
- La posterior gamma sobre λ no asigna probabilidad cero a tasas muy altas de fertilidad. Al muestrear λ de esa posterior y luego generar hijos con Poisson, permitimos que, en la cola, aparezcan valores mucho mayores que los observados.
- Hay que tener en cuenta las probabilidades de las colas (no el valor máximo aislado). No es lo mismo "posible" que "plausible".
:::

### Pregunta 18

-   Escribe una función que, dado un valor de la tasa de fertilidad $\lambda$ y un tamaño muestral $n$, simule **muestras de tamaño** $n$ de una distribución de Poisson y devuelva **un único número que sea el valor máximo** de dicha distribución. Ayúdate del prototipo de función que hay dentro del "callout".

::: {#respuesta-18 .callout-note}
```{r max-poisson}
max_poisson <- function(lambda, n) {
 # Simula n valores de una Poisson(λ)
  sims <- rpois(n, lambda)
  # Devuelve el máximo de la simulación
  max(sims)
}
```

Esta función:

1) Genera un vector de longitud *n* con *rpois(n, lambda)*.

2) Calcula y devuelve el valor máximo de ese vector con *max()*.

Por ejemplo, *max_poisson(3.5, 100)* devolvería el mayor número de hijos simulado en una muestra de 100 mujeres con tasa de fertilidad λ = 3.5.
:::

### Pregunta 19

-   Utilizando la aproximación a la distribución posterior de la pregunta 11 y la función `max_poisson()` que has escrito, determina el valor-$p$ predictivo posterior de obtener, según el modelo ajustado, una muestra de mujeres universitarias de tamaño `{r} n_con` en la que el máximo número de hijos sea igual o menor que `{r} max_hijos_emp` e interpreta el resultado.

*(NOTA: ¡Cuidado! Probablemente tengas que "iterar" sobre las muestras de la distribución posterior)*

::: {#respuesta-19 .callout-note}
1) Partir de la distribución posterior de la tasa de fertilidad $\lambda$ para el grupo con título universitario *lambda_con*.

2) Para cada posible $\lambda$ (cada simulación), aplicar *max_poisson()*:
  a. Simular una muestra de tamaño *n_con* de una Poisson$\lambda$.
  b. Calcula en esa muestra el máximo número de hijos.
  
3) Iterar todo ese proceso obteniendo una distribución simulada de máximos.

4) Comparar ese conjunto de máximos simulados con el valor observado en la muestra real.

5) El **valor-p predictivo posterior** es la proporción de simulaciones en que $\max_{\rm simulado}  \le  \max_{\rm empí­rico}$.

```{r valor-p.predictivo-posterior}
# Parámetros ya definidos
# lambda_con    : vector de simulaciones de la posterior Gamma para grupo de mujeres con titulación universitaria
# n_con         : tamaño de la muestra universitaria
# max_hijos_emp : máximo hijos observado en la muestra con titulación universitaria

# Simular el máximo para cada valor de lambda
max_sim <- map_int(lambda_con, ~ max_poisson(.x, n_con))

# Calcular el valor-p predictivo posterior
p_valor_predictivo <- mean(max_sim <= max_hijos_emp)

cat("Valor-p predictivo posterior  P(max ≤", max_hijos_emp, ") =",
    round(p_valor_predictivo, 3), "\n")
```

- Un valor-p predictivo de 0.434 significa que en el 43.4% de las replicas simuladas bajo el modelo, el máximo número de hijos es ≤ 4.
- Como está bastante cerca de 0.5, no hay evidencia de desajuste en este estadístico; el máximo observado es perfectamente compatible con lo que predice la posterior (solo si este valor-p fuese < 0.05 o > 0.95 hablaríamos de falta de ajuste).
:::

### Pregunta 20

-   En base a tus observaciones de las distribuciones predictivas posteriores, propón una comprobación predictiva posterior en alguna (o ambas) de las distribuciones en función de la titulación universitaria. Determina el valor-$p$ predictivo posterior correspondiente e interprétalo.

::: {#respuesta-20 .callout-note}
Evaluar si el modelo capta bien el *valor medio de hijos en el grupo con titulación universitaria* (estadístico $T(y)=\bar y_{\rm con}$, la media empírica de hijos en las 44 mujeres con titulación).

1) Del análisis bayesiano ya tenemos una muestra posterior de $\lambda_{\rm con}$ (vector *lambda_con*).

2) Para cada $T(y)=\bar y_{\rm con}^{(i)}$ simulamos una "réplica" de tamaño *n_con* de $\text{Poisson }T(y)=\bar y_{\rm con}^{(i)}$ y calculamos su media.

3) Repetimos para todas las simulaciones $\lambda_{\rm con}$ y almacenamos en *mean_sim*.

4) Calculamos la media empírica observada en la muestra real.

5) El **valor-p predictivo posterior** es la proporción de simulaciones en que P(media simulada ≥ media empírica).

```{r valor-p-predictivo-posterior}
# 2. y 3. Simular réplicas y calcular media
mean_sim <- map_dbl(lambda_con, ~ mean(rpois(n_con, .x)))

# 4. Estadístico empírico media
mean_emp_con <- fert_estadisticos %>%
  filter(titulo_uni == "con") %>%
  summarize(mean_emp = y / n) %>%
  pull(mean_emp)

# 5. Elegir el contraste unilateral según la dirección de la discrepancia
if (mean(mean_sim) < mean_emp_con) {
  # Si el modelo predice menos aceptaciones que las observadas
  p_val <- mean(mean_sim >= mean_emp_con)
  direccion <- "mayor"
} else {
  # Si el modelo predice más aceptaciones que las observadas
  p_val <- mean(mean_sim <= mean_emp_con)
  direccion <- "menor"
}

# Mostrar resultados
cat("Media predictiva:", mean(mean_sim), "\n")
cat("Media empírica (observada):", mean_emp_con, "\n")
cat("El valor observado es", direccion, "que lo esperado bajo el modelo\n")
cat("Valor-p predictivo posterior:", p_val, "\n")
```

- Un valor-p predictivo de 0.517 Significa que en el 51.7 % de las réplicas la media simulada es igual o superior a la empírica y indicaría un muy buen ajuste para la media de hijos del grupo de mujeres con título universitario ya que prácticamente es de 0.5.
:::
